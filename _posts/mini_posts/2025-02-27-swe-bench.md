---
layout: post
title: Качественный анализ SWE-bench
tags: [ai, benchmark]
tg_id: 596
---
[SWE-bench](https://www.swebench.com/#verified) — это бенчмарк на основе ~2300 реальных тикетов и связанных пулл-реквестов из 12 GitHub репозиториев на питоне. Его часто используют для того, чтобы заявить, что скоро нейронки заменят прогеров: например, недавний Claude 3.7 [показал](https://www.anthropic.com/news/claude-3-7-sonnet) в нем аж 70% (в то время как у текущих рекордсменов — 49%). Бенчмарк изначально задумывался как способ проверки решения "реальных" проблем в реальных проектах.

Вообще, вопросы к репрезентативности должны возникнуть уже на стадии более подробного описания/[аннотации](https://arxiv.org/abs/2310.06770) (только питон, 12 реп и т.д.), но для своего времени (полтора года назад всего) бенчмарк был хорош — лучшая модель в нем набирала меньше 2%. А вот год спустя появилась [статья](https://arxiv.org/abs/2410.06992) с качественным анализом бенчмарка.

Результаты получились довольно яркие: 
* в 33% случаев решение можно было довольно тривиально вывести из условия (например, решение было предоставленно в комментарии к тикету);
* в 31% решений были хреновые тесты (и "решение" хоть и проходило тесты, но было неполным/некорректным);
* 94% тикетов были созданы до даты отсечки знаний LLM (т.е. датасет нейронки потенциально мог включать в себя PR, закрывающий тикет);
* для всех задач были тесты -_-

Соответственно, если улучшить тесты/условия, то и проценты в бенчмарках будут на порядок ниже. Вот такие маркетинговые пироги :)
